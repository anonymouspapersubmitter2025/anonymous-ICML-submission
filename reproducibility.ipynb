{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e4bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install ray\n",
    "!pip install optuna\n",
    "!pip install sentence-transformers\n",
    "!pip install tqdm\n",
    "!pip install dimod\n",
    "!pip install dwave-neal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefa6748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import train, tune\n",
    "import optuna\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tqdm\n",
    "import math\n",
    "import dimod, neal, time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def formattedReasons(raw_res, rounder=0):\n",
    "    ret_d=dict()\n",
    "    not_pref=\"N*: \"\n",
    "    tot_sum=0\n",
    "    for r_res in raw_res:\n",
    "        weight_v = int(r_res.split(\"_\")[-1])\n",
    "        access_f=\"_\".join(r_res.split(\"_\")[:-1])\n",
    "        tot_sum+=math.pow(2, weight_v)\n",
    "        if access_f in ret_d:\n",
    "            ret_d[access_f]+=math.pow(2, weight_v)\n",
    "        else:\n",
    "            ret_d[access_f]=math.pow(2, weight_v)\n",
    "    tmp_l=[]\n",
    "    for d_key, d_val in ret_d.items():\n",
    "        tmp_val = d_val/tot_sum\n",
    "        if d_key[:len(not_pref)]==not_pref:\n",
    "            tmp_val*=-1\n",
    "            d_key=d_key[len(not_pref):]\n",
    "        if rounder:\n",
    "            tmp_val=round(tmp_val, rounder)\n",
    "        if abs(tmp_val)>0:\n",
    "            tmp_l.append(\"(\"+str(tmp_val)+\") \"+d_key)\n",
    "    return sorted(tmp_l, reverse=True)\n",
    "\n",
    "def simulatedAnnealing(BQM, n_sweeps, n_reads):\n",
    "    sampler = neal.SimulatedAnnealingSampler()\n",
    "    start_time=time.perf_counter()\n",
    "    samples = sampler.sample(BQM, num_sweeps = n_sweeps, num_reads = n_reads) \n",
    "    end_time=time.perf_counter()\n",
    "    time_spent=end_time-start_time\n",
    "\n",
    "    # getting best reasons selected\n",
    "    soln = np.array(list(samples.first.sample.values()))\n",
    "    best_reasons_DWSA = [k for k, v in samples.first.sample.items() if v == 1]\n",
    "    best_energy = samples.first.energy\n",
    "    return best_reasons_DWSA, time_spent, best_energy\n",
    "\n",
    "def decode(prompt, systemInstruction, temperature, numSamples, model = \"gpt-3.5-turbo\", maxTokens = 350):\n",
    "    # please add your api_key below\n",
    "    client = OpenAI(api_key = \"ADD KEY HERE\")\n",
    "    if isinstance(prompt, list): \n",
    "        message = [{\"role\": \"user\", \"content\": prompt[i]} for i in range(len(prompt))]\n",
    "    else:\n",
    "        message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    if systemInstruction != None:\n",
    "         message = [{\"role\": \"system\", \"content\": systemInstruction}] + message\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages= message,\n",
    "      n = numSamples,\n",
    "      temperature = temperature,\n",
    "      max_tokens = maxTokens,\n",
    "    )\n",
    "    return [completion.choices[i].message.content for i in range(numSamples)]\n",
    "\n",
    "def formatResults(path):\n",
    "    tasks = [f[:-5] for f in listdir(\"bbhFiles/\") if isfile(join(\"bbhFiles/\", f))]\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    results = dict()\n",
    "    for i in range(len(tasks)):\n",
    "        indices = df['Dataset'] == tasks[i]\n",
    "        results[tasks[i]] = df[\"Correctness\"][indices].sum()/indices.sum()\n",
    "\n",
    "    results[\"tracking_shuffled_objects\"] = np.mean([results[task] for task in tasks if task[:8] == \"tracking\"])\n",
    "    results[\"logical_deduction\"] = np.mean([results[task] for task in tasks if task[:7] == \"logical\"])\n",
    "    results = { k:v for k, v in results.items() if (k[:26] != \"tracking_shuffled_objects_\" and k[:18] != \"logical_deduction_\")}\n",
    "    return results\n",
    "\n",
    "remoteDecode = ray.remote(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8503d40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class combinatorialReasoner:\n",
    "    def __init__(self):\n",
    "        self.simThreshold = 0.9\n",
    "        self.model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "    def sample(self, dataset, questionNo, forceResample = False):\n",
    "        if not os.path.isfile(f\"bbhFiles/{dataset}/question_{questionNo}.npy\") or forceResample:\n",
    "            print(\"Sampling Question\")\n",
    "            data = json.load(open(\"bbhFiles/\" + dataset + \".json\"))\n",
    "            question = data[\"examples\"][questionNo][\"input\"]\n",
    "\n",
    "            resultIDs = [remoteDecode.remote([\"Output template: \\nStep 1: reasoning \\n{condensed reason}\\nStep 2: reasoning \\n{condensed reason} ....\", question],\n",
    "                                        \"Let's think step by step. After each step, condense the reasoning in the step into a sentence and put it in curly braces.\", \n",
    "                                        temperature = 1., \n",
    "                                        numSamples = 70,\n",
    "                                        maxTokens = 1000)\n",
    "                                        for i in range(3)]\n",
    "            results = np.array(ray.get(resultIDs))\n",
    "            np.save(f\"bbhFiles/{dataset}/question_{questionNo}.npy\", results)\n",
    "        \n",
    "        res = np.load(f\"bbhFiles/{dataset}/question_{questionNo}.npy\")\n",
    "        reasonSamples = {}\n",
    "        for i in range(res.shape[0]):\n",
    "            for j in range(res.shape[1]):\n",
    "                a = re.findall(r'\\{.*?\\}', res[i][j])\n",
    "                a = [k for k in a if ((len(k) > 15))]\n",
    "                if a:\n",
    "                    reasonSamples[(i, j)] = a\n",
    "        return reasonSamples, res\n",
    "\n",
    "    def answer(self, hyperparams, dataset, questionNo):\n",
    "        if not isfile(f\"QUBOs/{dataset}/qubo_{questionNo}.npy\"):\n",
    "            raise Exception(\"Cannot find the QUBO for this question, please check the path.\")\n",
    "        \n",
    "        BQM = np.load(f\"QUBOs/{tasks[i]}/qubo_{testing[i, j]}.npy\", allow_pickle = True).item()\n",
    "        bestReasons, timeSpent, bestEnergy = simulatedAnnealing(BQM, n_sweeps = 1000, n_reads = 100) # Choose number of sweeps and reads here\n",
    "        weightedBestReasons = formattedReasons(bestReasons, 3)\n",
    "\n",
    "        data = json.load(open(\"bbh_testing/bbhFiles/\" + dataset + \".json\"))\n",
    "        question = data[\"examples\"][questionNo][\"input\"]\n",
    "        answer = data[\"examples\"][questionNo][\"target\"]\n",
    "        prompt = \"Q: \" + question + \"\\nW-Statements:\\n\" + \"\\n\".join(weightedBestReasons)\n",
    "        systemInstruction =\"Each W-Statement starts with the substring (w), where (w) is a number called the W-Value. Identify and state each W-Value. W-Statements with higher W-Values have more reliable information. You may not provide multiple possible answers, you must narrow your final solution down to a single answer. Refer to each W-Statement and their W-Values in your reasoning. Your final answer must be of the form SOLUTION: (option).\"\n",
    "        response = decode(prompt, systemInstruction, temperature=0., numSamples = 1, maxTokens=500)[0]\n",
    "        if answer in response.split(\"SOLUTION:\")[-1]:\n",
    "            return [1, response, answer, dataset, questionNo, hyperparams, bestReasons, bestEnergy, timeSpent, prompt]\n",
    "        else:\n",
    "            return [0, response, answer, dataset, questionNo, hyperparams, bestReasons, bestEnergy, timeSpent, prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3036c7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1350 [00:21<08:28,  2.62it/s] /tmp/ipykernel_22241/3928084537.py:19: RuntimeWarning: invalid value encountered in sqrt\n",
      "  cur_risk_lookup[tmp_n]=np.sqrt((1-cur_val)*cur_val)*risk_p\n",
      "100%|██████████| 1350/1350 [15:07<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tasks = ['causal_judgement',\n",
    "         'reasoning_about_colored_objects',\n",
    "         'navigate',\n",
    "         'penguins_in_a_table',\n",
    "         'geometric_shapes',\n",
    "         'disambiguation_qa',\n",
    "         'tracking_shuffled_objects_five_objects',\n",
    "         'word_sorting',\n",
    "         'tracking_shuffled_objects_three_objects',\n",
    "         'tracking_shuffled_objects_seven_objects',\n",
    "         'multistep_arithmetic_two',\n",
    "         'web_of_lies',\n",
    "         'logical_deduction_three_objects',\n",
    "         'sports_understanding',\n",
    "         'snarks',\n",
    "         'logical_deduction_five_objects',\n",
    "         'salient_translation_error_detection',\n",
    "         'hyperbaton',\n",
    "         'movie_recommendation',\n",
    "         'object_counting',\n",
    "         'logical_deduction_seven_objects',\n",
    "         'temporal_sequences',\n",
    "         'formal_fallacies',\n",
    "         'dyck_languages',\n",
    "         'date_understanding',\n",
    "         'boolean_expressions',\n",
    "         'ruin_names']\n",
    "testing = np.load(\"bbhQuestionNumbers.npy\")\n",
    "cr = combinatorialReasoner()\n",
    "\n",
    "iterator = list(itertools.product(range(27), range(50)))\n",
    "\n",
    "# uncomment below line if you want to randomly iterate through the evaluation set\n",
    "#random.shuffle(iterator)\n",
    "\n",
    "records = []\n",
    "for i, j in tqdm.tqdm(iterator):\n",
    "    hyperparams = {'linearSensitivity': 53.1543,\n",
    "               'threshParam': -1.90906,\n",
    "               'riskParam': 1.37128,\n",
    "               'weight': 2}\n",
    "    record = cr.answer(hyperparams, tasks[i], testing[i, j])\n",
    "    records. append(record)\n",
    "\n",
    "pathToResults = \"reproducitbility.csv\"\n",
    "df = pd.DataFrame(records, columns = [\"Correctness\", 'LLM Response', 'Answer', 'Dataset', 'Question Number', \"Hyperparameters\", \"Best Reasons\", \"Best Energy\", \"Time Spent\", \"LLM Prompt\"])\n",
    "df.to_csv(pathToResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dictionary of automatically parsed accuracies. Will be correct for most multiple choice datasets,\n",
    "    but fails for dyck languages, word sorting, and a few others\"\"\"\n",
    "\n",
    "crResultsDict = formatResults(pathToResults)\n",
    "crResultsDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (all)",
   "language": "python",
   "name": "all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
